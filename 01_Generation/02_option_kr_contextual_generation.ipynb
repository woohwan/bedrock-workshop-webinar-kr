{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BedRock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "\n",
      "== FM list ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-G1': 'TBD'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from termcolor import colored\n",
    "from pprint import  pprint\n",
    "from utils.bedrock import bedrock_info # class method\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "\n",
    "boto3_bedrock_cli = bedrock.get_bedrock_client(\n",
    "  assumed_role = os.environ.get(\"BEDROCK_ASSUME_ROLE\",None),\n",
    "  endpoint_url = os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "  region = os.environ.get(\"AWS_DEFAULT_REGION\",None)\n",
    ")\n",
    "print(colored(\"\\n== FM list ==\"))\n",
    "pprint(bedrock_info.get_list_fm_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LangChain Integration을 사용하여 Bedrock Client 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "inference_modifier = {\n",
    "  'max_tokens_to_sample': 4096,\n",
    "  'temperature': 0.5,\n",
    "  'top_k': 250,\n",
    "  'top_p': 1,\n",
    "  'stop_sequences': [\"\\n\\nHuman\"]\n",
    "}\n",
    "\n",
    "textgen_llm = Bedrock(\n",
    "  model_id = 'anthropic.claude-v2',\n",
    "  client = boto3_bedrock_cli,\n",
    "  model_kwargs = inference_modifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. LangChain 사용자 정의 프롬프트 템플릿 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "multi_var_prompt = PromptTemplate(\n",
    "  input_variables=[\"customerServiceManager\", \"customerName\", \"feedbackFromCustomer\"],\n",
    "  template=\"\"\"서비스 관리자 {customerServiceManager}가 {customerName}에게 보내는 사과 이메일을 작성합니다.\n",
    "  고객으로부터 받은 다음 피드백에 대한 응답: {feedbackFromCustomer}\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(customerServiceManager=\"권율\",\n",
    "                                  customerName=\"이순신\",\n",
    "                                  feedbackFromCustomer=\"\"\"안녕하세요 권율님,\n",
    "     귀하의 고객 지원팀에 전화했을 때의 최근 경험에 매우 실망했습니다.\n",
    "     나는 즉시 전화를 받을 것으로 예상했지만 전화를 받는 데 3일이 걸렸습니다.\n",
    "     문제를 해결하기 위한 첫 번째 제안이 올바르지 않았습니다. 결국 문제는 3일 만에 해결 되었습니다.\n",
    "     우리는 제공된 응답에 매우 만족하지 않으며 다른 곳에서 비즈니스를 수행하는 것을 고려할 수 있습니다. \n",
    "     \"\"\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      " 서비스 관리자 권율가 이순신에게 보내는 사과 이메일을 작성합니다.\n",
      "  고객으로부터 받은 다음 피드백에 대한 응답: 안녕하세요 권율님,\n",
      "     귀하의 고객 지원팀에 전화했을 때의 최근 경험에 매우 실망했습니다.\n",
      "     나는 즉시 전화를 받을 것으로 예상했지만 전화를 받는 데 3일이 걸렸습니다.\n",
      "     문제를 해결하기 위한 첫 번째 제안이 올바르지 않았습니다. 결국 문제는 3일 만에 해결 되었습니다.\n",
      "     우리는 제공된 응답에 매우 만족하지 않으며 다른 곳에서 비즈니스를 수행하는 것을 고려할 수 있습니다. \n",
      "     \n"
     ]
    }
   ],
   "source": [
    "print(\"prompt: \\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human:서비스 관리자 권율가 이순신에게 보내는 사과 이메일을 작성합니다.\n",
      "  고객으로부터 받은 다음 피드백에 대한 응답: 안녕하세요 권율님,\n",
      "     귀하의 고객 지원팀에 전화했을 때의 최근 경험에 매우 실망했습니다.\n",
      "     나는 즉시 전화를 받을 것으로 예상했지만 전화를 받는 데 3일이 걸렸습니다.\n",
      "     문제를 해결하기 위한 첫 번째 제안이 올바르지 않았습니다. 결국 문제는 3일 만에 해결 되었습니다.\n",
      "     우리는 제공된 응답에 매우 만족하지 않으며 다른 곳에서 비즈니스를 수행하는 것을 고려할 수 있습니다. \n",
      "     \n",
      "  \n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "HUMAN = \"\\n\\nHuman:\"\n",
    "ASSISTANT = \"\\n\\nAssistant:\"\n",
    "prompt = HUMAN + prompt + ASSISTANT\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prompt has 274 tokens\n"
     ]
    }
   ],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. PROMPT 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "저는 최근 고객님과의 상호작용에서 발생한 불편을 진심으로 사과드립니다. 고객님께 신속하고 정확한 서비스를 제공하는 것이 우리의 최우선 과제입니다.\n",
      "\n",
      "3일이 넘게 기다려야 했다는 점과 처음 제안한 해결책이 적절하지 않았다는 점에서 고객님께 실망을 드린 점 진심으로 죄송합니다. 이는 우리의 서비스 수준에 대한 고객님의 기대에\n",
      "미치지 못한 것이며, 저희 서비스 품질 개선을 위해 귀중한 피드백으로 여기고 있습니다.\n",
      "\n",
      "앞으로 이러한 일이 다시 발생하지 않도록 문제 해결 프로세스를 개선하고, 고객님의 요구에 더욱 신속하고 정확하게 대응할 수 있는 교육을 직원들에게 시행하겠습니다.\n",
      "\n",
      "고객님의 지속적인 성원에 감사드리며, 더 나은 서비스로 보답할 수 있도록 노력하겠습니다. 언제든지 피드백을 주십시오.\n",
      "\n",
      "감사합니다.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = textgen_llm(prompt)\n",
    "email = response[response.index('\\n')+1:]\n",
    "print_ww(colored(email, 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
