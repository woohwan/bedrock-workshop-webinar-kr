{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet \"faiss-cpu>=1.7,<2\" \"ipywidgets>=7,<8\" \"pypdf>=3.8,<4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bedrock client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock, print_ww\n",
    "from utils.bedrock import bedrock_info\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "  assumed_role=None,\n",
    "  endpoint_url=None,\n",
    "  region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 챗봇(기본 - 컨텍스트 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chat import chat_utils\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create the Anthropic Model\n",
    "llm_text = Bedrock(\n",
    "  model_id=\"anthropic.claude-v2\",\n",
    "  client=boto3_bedrock,\n",
    "  model_kwargs={\n",
    "    \"max_tokens_to_sample\": 512,\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.999,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "  },\n",
    "  streaming=True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory= chat_utils.get_memory(\n",
    "  memory_type=\"ConversationBufferMemory\",\n",
    "  memory_key='history'\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "  llm=llm_text,\n",
    "  verbose=True,\n",
    "  memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "  response = conversation.predict(input=\"안녕하세요?\")\n",
    "else:\n",
    "  print_ww(conversation.predict(input=\"안녕하세요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_utils.clear_memory(\n",
    "  chain=conversation\n",
    ")\n",
    "print_ww(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 프롬프트 템플릿(Langchain)을 이용한 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn verbose to true to se the full logs and documents\n",
    "conversation = ConversationChain(\n",
    "  llm=llm_text,\n",
    "  verbose=False,\n",
    "  memory=memory\n",
    ")\n",
    "\n",
    "claude_prompt = PromptTemplate(\n",
    "  input_variables = [\"history\", \"input\"],\n",
    "  template=\"\"\"\n",
    "  \\n\\nHuman: Here's a friendly conversation between a user and an AI.\n",
    "  The AI is talkative and provides lots of contextualized details.\n",
    "  if it doesn't know. it will honestly say that it doesn't know the answer to the question.\n",
    "\n",
    "  Current convesation:\n",
    "  {history}\n",
    "\n",
    "  \\n\\nUser: {input}\n",
    "\n",
    "  \\n\\nAssistant:\n",
    "  \"\"\"\n",
    ")\n",
    "print(\"claude_prompt: \\n\", claude_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.prompt = claude_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_utils.get_tokens(\n",
    "  chain=conversation,\n",
    "  prompt=\"안녕하세요?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "  response = conversation.predict(input=\"안녕하세요\")\n",
    "else:\n",
    "  print_ww(conversation.predict(input=\"안녕하세요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "  response = conversation.predict(input=\"좋은 고기를 고르는 몇 가지 팁을 알려 주세요\")\n",
    "else:\n",
    "  print_ww(conversation.predict(input=\"좋은 고기를 고르는 몇 가지 팁을 알려 주세요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문을 토대로 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 이전 대화를 이해할 수 있는 지 확인하기 위해 정원이라는 단어를 언급하지 않고 질문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "  response = conversation.predict(input=\"좋아요. 생선에도 적용될까요?\")\n",
    "else:\n",
    "  print_ww(conversation.predict(input=\"좋아요. 생선에도 적용될까요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ww(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 대화 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "  response = conversation.predict(input=\"좋은 정보 감사합니다. 고마워요!\")\n",
    "else:\n",
    "  print_ww(conversation.predict(input=\"좋은 정보 감사합니다. 고마워요!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "  \"\"\"A chat UX using IPYWidgets\"\"\"\n",
    "  def __init__(self, qa, retrivealChain = False) -> None:\n",
    "    self.qa = qa\n",
    "    self.name = None\n",
    "    self.b = None\n",
    "    self.retrievalChain = retrivealChain\n",
    "    self.out = ipw.Output()\n",
    "\n",
    "    if \"ConversationChain\" in str(type(self.qa)):\n",
    "      self.streaming = self.qa.llm.streaming\n",
    "    elif \"ConversationalRetrievalChain\" in str(type(self.qa)):\n",
    "      self.streaming = self.qa.combine_docs_chain.llm_chain.llm.streaming\n",
    "\n",
    "  def start_chat(self):\n",
    "    print(\"Starting chat bot\")\n",
    "    display(self.out)\n",
    "    self.chat(None)\n",
    "\n",
    "  def chat(self, _):\n",
    "    if self.name is None:\n",
    "      prompt = \"\"\n",
    "    else:\n",
    "      prompt = self.name.value\n",
    "    if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "      print(\"Thank you, that was a nice chat !!\")\n",
    "      return\n",
    "    elif len(prompt):\n",
    "      with self.out:\n",
    "        thiking = ipw.Label(value=\"Thinking...\")\n",
    "        display(thiking)\n",
    "        try:\n",
    "          if self.retrievalChain:\n",
    "            result = self.qa.run({'question': prompt})\n",
    "          else:\n",
    "            result = self.qa.run({'input': prompt})\n",
    "        except:\n",
    "          result = \"No answer\"\n",
    "        thiking.value = \"\"\n",
    "        if self.streaming:\n",
    "          response = f\"AI:{result}\"\n",
    "        else:\n",
    "          print_ww(f\"AI:{result}\")\n",
    "        self.name.disabled = True\n",
    "        self.b.disabled = True\n",
    "        self.name=None\n",
    "    \n",
    "    if self.name is None:\n",
    "      with self.out:\n",
    "        self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "        self.b = ipw.Button(description=\"Send\")\n",
    "        self.b.on_click(self.chat)\n",
    "        display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat =ChatUX(conversation)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 페르소나를 활용한 챗봇\n",
    "AI비서가 커리어 코치 역할을 하게 됩니다.\n",
    "- 역할극 대화에서는 채팅을 시작하기 전에 사용자ㅏ 메시지를 설정해야 합니다. CoversationBufferMemory는 대화 상자를 미리 채우는데 사용됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) ConversationChain 생성 필요한 메모리 초기화, Bedrock Claude 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "\n",
    "llm_text = Bedrock(\n",
    "  model_id=\"anthropic.claude-v2\",\n",
    "  client=boto3_bedrock,\n",
    "  model_kwargs={\n",
    "    \"max_tokens_to_sample\": 1000,\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.999,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "  },\n",
    "  streaming=True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# memory\n",
    "# store previous interactions using ConversationalBufferMemory and add custom prompts to the that\n",
    "memory = chat_utils.get_memory(\n",
    "  memory_type=\"ConversationBufferMemory\",\n",
    "  memory_key=\"history\"\n",
    ")\n",
    "\n",
    "memory.chat_memory.add_user_message(\"당신은 직업 코치로 활동하게 될 것입니다. 귀하의 목표는 사용자에게 직업 조언을 제공하는 것입니다.\")\n",
    "memory.chat_memory.add_ai_message(\"나는 직업 코치이며 직업에 대한 조언을 제공합니다.\")\n",
    "\n",
    "# conversation chain\n",
    "conversation = ConversationChain(\n",
    "  llm=llm_text,\n",
    "  verbose=True,\n",
    "  memory=memory\n",
    ")\n",
    "\n",
    "print(conversation)\n",
    "\n",
    "# langchain prompts do not always work with the models. This prompt is tuned for Claude\n",
    "claude_prompt = PromptTemplate.from_template(\"\"\"\n",
    "\\n\\nHuman: Here's a friendly conversation between a user and AI.\n",
    "The AI is talkative and provides lots of contextualized details.\n",
    "If it doesn't know, it will honestly say that it doesn't know the answer to the question.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "User: {input}\n",
    "\n",
    "\\n\\nAssistant:\n",
    "\"\"\")\n",
    "\n",
    "print(\"claude_prompt: \\n\", claude_prompt)\n",
    "conversation.prompt = claude_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "  response = conversation.predict(input=\"인공지능에 관련된 직업은 어떤 것이 있습니까?\")\n",
    "else:\n",
    "  print_ww(conversation.predict(input=\"인공지능에 관련된 직업은 어떤 것이 있습니까?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_text.streaming:\n",
    "    response = conversation.predict(input=\"이 직업들은 실제로 무엇을 하는가요? 재미있나요?\")\n",
    "else:\n",
    "    print_ww(conversation.predict(input=\"이 직업들은 실제로 무엇을 하는가요? 재미있나요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 맥락을 가진 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상황에 맞는 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 사용 사례에서는 Chatbot에게 이전에 본 적이 없는 외부 코퍼스의 질문에 답변하도록 요청합니다. 이를 위해 RAG(Retrieval Augmented Generation)\n",
    "이라는 패턴을 적용합니다. 아이디어는 말뭉치를 덩어리로 인덱싱한 다음 덩어리와 질문사이의 의미론적 유사성을 사용하여 말뭉치의 어느 섹션이 답변을 제공\n",
    "하는 데 관련될 수 있는 지 찾는 것입니다. 마지막으로 가장 관련성이 높은 청크가 집계되어 기록을 제공하는 것과 유사하게 ConversationChain에 컨텍스트로\n",
    "전달됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 2022년"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "  client=boto3_bedrock,\n",
    "  model_id=bedrock_info.get_model_id(\n",
    "    model_name=\"Titan-Embeddings-G1\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPDFDirctoryLoader를 통한 PDF 파일 로딩\n",
    "- chunk_size는 임베딩 모델의 최대 토큰인 512를 고려해서 정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 위치\n",
    "loader = PyPDFDirectoryLoader(\"./rag_data_kr_pdf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading documents and split \n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=230,\n",
    "  chunk_overlap=50,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to memory using FAISS\n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "  documents = docs,\n",
    "  embedding = bedrock_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) / len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"docs[0].paget_content: \\n\", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search\n",
    "LangChain에서 제공하는 Wrapper 클래스를 사용하여 벡터 데이터베이스 저장소를 쿼리하고 관련 문서를 변환.<br>\n",
    "뒤에서는 RetrievalQA체인만 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"아마존은 Generative AI의 전략은 무엇인가요\"\n",
    "print_ww(wrapper_store_faiss.query(query, llm=llm_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의미론적 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = bedrock_embeddings.embed_query(query)\n",
    "print(v[0:10])\n",
    "results = vectorstore_faiss_aws.similarity_search_by_vector(v, k=3)\n",
    "for r in results:\n",
    "  print_ww(r.page_content)\n",
    "  print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리\n",
    "모든 챗봇에는 사용 사례에 따라 맞춤화된 다양한 옵션을 갖춘 QA 체인이 필요합니다. 그러나, 챗봇에서는 모델이 답변을 제공하기 위해 이를 고려할 수 있도록\n",
    "항상 대화 기록을 보관해야 합니다. 이 예에서는 대화 기록을 유지하기 위해 ConversationBufferMemory와 함께 LangChain의 ConversationalRetrievalChain을\n",
    "사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "print_ww(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationalRetrievalChain에 사용되는 매개변수\n",
    "- retriever: 우리는 VectorStore가 지원하는 VectorStoreRetriever를 사용. 텍스트를 검색하려면 \"similarity\" 또는 \"mmr\"라는 두 가지 검색 유형 선택\n",
    "  search_type=\"similarity\"는 질문 벡터와 가장 유사한 텍스트 청크 벡터를 선택하는 검색 객체에서 유사성 검색 사용\n",
    "- 메모리: 이력을 저장하는 메모리 체인\n",
    "- dense_question_prompt: 사용자의 질문이 주어지면 이전 대화와 해당 질문을 사용하여 독립형 질문을 구성\n",
    "- chain_type: 채팅 기록이 길고 상황에 맞지 않는 경우, 이 매개 변수를 사용하고 stuff, refine, map_reduce, map_relank가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "  llm=llm_text,\n",
    "  retriever=vectorstore_faiss_aws.as_retriever(),\n",
    "  memory=memory_chain,\n",
    "  condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "  chain_type='stuff'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(qa, retrivealChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 랭체인 프롬프트는 Claude에게 최적이 아닙니다. 다음 셀에서는 두 개의 새로운 프롬프트 설정\n",
    "하나는 질문의 표현을 변경하기 위한 것이고, 다른 하나는 해당 질문의 답변을 얻기 위한 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "\n",
    "# We are also providing a different chat history retriever which outputs the history as a Claude chat (ie including the \\n\\n)\n",
    "_ROLE_MAP = {\"human\": \"\\n\\nHuman: \", \"ai\": \"\\n\\nAssistant: \"}\n",
    "def _get_chat_history(chat_history):\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        if isinstance(dialogue_turn, BaseMessage):\n",
    "            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f\"{dialogue_turn.type}: \")\n",
    "            buffer += f\"\\n{role_prefix}{dialogue_turn.content}\"\n",
    "        elif isinstance(dialogue_turn, tuple):\n",
    "            human = \"\\n\\nHuman: \" + dialogue_turn[0]\n",
    "            ai = \"\\n\\nAssistant: \" + dialogue_turn[1]\n",
    "            buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported chat history format: {type(dialogue_turn)}.\"\n",
    "                f\" Full chat history: {chat_history} \"\n",
    "            )\n",
    "    return buffer\n",
    "\n",
    "# the condense prompt for Claude\n",
    "condense_prompt_claude = PromptTemplate.from_template(\"\"\"{chat_history}\n",
    "\n",
    "새로운 질문으로만 대답하세요.\n",
    "\n",
    "\n",
    "Human: 이전 대화를 고려하여 질문을 어떻게 하시겠습니까?: {question}\n",
    "\n",
    "\n",
    "Assistant: Question:\"\"\")\n",
    "\n",
    "# recreate the Claude LLM with more tokens to sample - this provide longer response but introduces some latency\n",
    "cl_llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 500})\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cl_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 8}),\n",
    "    memory=memory_chain,\n",
    "    get_chat_history=_get_chat_history,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=condense_prompt_claude, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")\n",
    "\n",
    "# the LLMChain prompt to get the answer. \n",
    "qa.combine_docs_chain.llm_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "{context}\n",
    "\n",
    "Human: <q></q> XML 태그내의 질문에 답하려면 최대 3개의 문장을 사용하세요.\n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "답변에 XML태그를 사용하지 마세요. 답변이 context에 없으면 \"죄송합니다. 문맥에서 찾을 수 없어서 모르겠습니다.\"라고 말합니다.\n",
    "\n",
    "Assistant:\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "채팅을 해보죠. 아래와 같은 질문을 해보세요.\n",
    "1. 아마존의 Generative AI 의 전략이 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(qa, retrivealChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
