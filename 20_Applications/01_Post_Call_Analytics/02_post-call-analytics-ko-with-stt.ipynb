{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df503520-00ac-434c-aea7-458add4857d9",
   "metadata": {},
   "source": [
    "## Post Call Analytics (PCA) Using Amazon Bedrock\n",
    "Amazon Bedrock을 사용한 통화 후 분석 사용 사례에 대한 이 교육에 오신 것을 환영합니다.\n",
    "\n",
    "기업이 다양한 채널을 통해 고객과 계속 상호 작용함에 따라 이러한 상호 작용을 분석하여 고객 행동 및 선호도에 대한 인사이트를 얻는 것이 점점 더 중요해지고 있습니다.<BR>\n",
    "통화 후 분석은 통화 종료 후 고객 상호 작용을 분석하는 방법 중 하나입니다.<BR>\n",
    "대규모 언어 모델을 사용하면 보다 정확한 감정 분석을 가능하게 하고, 특정 고객의 요구와 선호도를 파악하며, 전반적인 고객 경험을 개선함으로써 통화 후 분석의 효과를 크게 높일 수 있습니다.<BR>\n",
    "\n",
    "이 샘플 노트북에서는 통화 후 분석에 Bedrock을 사용할 때 얻을 수 있는 다양한 이점과 기업이 현대 시장에서 경쟁 우위를 확보할 수 있는 방법을 다음 주제를 통해 살펴봅니다.<BR>\n",
    "\n",
    "- 베드락에서 LLM 모델 선택(타이탄 텍스트 및 앤트로닉 클로드)\n",
    "- 하나의 모델로 여러 PCA 작업 처리\n",
    "- 긴 통화 기록 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91f900-495c-4e06-ae35-15b68d53a8a3",
   "metadata": {},
   "source": [
    "# 0. Auto Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb14c0-0b39-4001-8594-bab215bce4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4af435-2f0b-47cf-849c-fc16f8189400",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb7a76-5b2e-4b42-99e2-7be0e926af2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"../../\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba4080-2f28-405b-b541-8853f7c96121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import librosa\n",
    "import langchain\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from time import strftime\n",
    "from termcolor import colored\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import soundfile as sf\n",
    "from utils import bedrock\n",
    "from utils.s3 import s3_handler\n",
    "from urllib.request import urlopen\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d743152-ae17-4ffe-af52-a1febdafdb90",
   "metadata": {},
   "source": [
    "# 2. Speech To Text (STT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943fdde-af99-4af2-b21a-cc5e1f51ab07",
   "metadata": {},
   "source": [
    "## 2.1. Run audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57baff1-789c-4cfa-acc2-8ec845b2d51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipd.Audio(\"./records/voice-examples.wav\", autoplay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a3239-2cfe-48e0-8e51-4df5b660a8c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2. Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cad292-1f12-4815-abe4-de0b995eb8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = s3_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b398db-2473-4642-a5a5-c7686a4434b1",
   "metadata": {},
   "source": [
    "#### [중요] 아래 prefix 에 본인의 이름을 넣어 주세요. (예: gonsoomoon)\n",
    "- bucket_name 이름은 유니크하기에, 다른 곳에서 사용을 하면 생성이 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebad69-c0a1-470c-8cc9-6fe5d826efd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = <your-id> #\" gonsoomoon, mx-40\"\n",
    "bucket_name = f'bedrock-training-{prefix}'\n",
    "data_dir = \"./records\"\n",
    "data_path_s3 =f's3://{bucket_name}/records/voice-examples.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541d0ef-169a-4ddb-bfa3-fbaea6817376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3.create_bucket(bucket_name)\n",
    "source_dir, target_bucket, target_dir = data_dir, bucket_name, \"/records\"\n",
    "s3.upload_dir(source_dir, target_bucket, target_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5267f-ef3c-46ff-ba12-ed7d91af87e7",
   "metadata": {},
   "source": [
    "## 2.3. Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e1402-5032-45fb-9182-ba66e5fa2ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client('transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa424d8-1ccd-47ee-89fc-ff75cc5ad2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_date = strftime(\"%m%d-%H%M%s\")\n",
    "job_name = f'{prefix}-stt-job-{create_date}'\n",
    "print (f's3 data path: {data_path_s3}')\n",
    "print (f'job_name: {job_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7690b9-a241-4097-931c-ddb6ec12eba1",
   "metadata": {},
   "source": [
    "### 2.3.1. Run Transcribe Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28543a60-d811-45c2-a6a1-d22e944e3e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcribe_client.start_transcription_job(\n",
    "    TranscriptionJobName=job_name,\n",
    "    Media={'MediaFileUri': data_path_s3},\n",
    "    MediaFormat='wav',\n",
    "    Settings={\n",
    "        'ShowSpeakerLabels': True,\n",
    "        'MaxSpeakerLabels': 2,\n",
    "    },\n",
    "    LanguageCode='ko-KR'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3dd4ad-76a4-4138-9ad3-a4c724c535f8",
   "metadata": {},
   "source": [
    "### 2.3.2. Check Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7d427-3fb3-47c9-b3e6-994b594be648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    status = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n",
    "    if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(10)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdf2e7-deed-4eb3-8e77-ca43a69a4d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = urlopen(status['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
    "data = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa89488-6243-48bd-a40d-b58f374ad33a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3.3. Retrieve Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7aad72-d825-4003-aa78-433c66b3069a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spk_sperator(data):\n",
    "\n",
    "    previos_spk = \"\"\n",
    "    contents, contents_temp = [], []\n",
    "    end_time = None\n",
    "\n",
    "    for res in data[\"results\"][\"items\"]:\n",
    "        #print (res)\n",
    "        speaker_label = res[\"speaker_label\"]\n",
    "        content = res[\"alternatives\"][0][\"content\"]\n",
    "        start_time = res.get(\"start_time\", None)\n",
    "\n",
    "        if previos_spk != speaker_label:\n",
    "\n",
    "            contents_temp.append(f'<종료시간:{end_time}>')\n",
    "            contents.append(\" \".join(contents_temp))\n",
    "            contents_temp = []\n",
    "\n",
    "            contents_temp.append(f'{speaker_label}:<시작시간:{start_time}>')\n",
    "            contents_temp.append(content)\n",
    "        else:\n",
    "            contents_temp.append(content)\n",
    "            if content not in [\"?\", \",\", \".\"]: end_time = res.get(\"end_time\", None)\n",
    "\n",
    "        previos_spk = speaker_label\n",
    "\n",
    "    contents_temp.append(f'<종료시간:{end_time}>')\n",
    "    contents.append(\" \".join(contents_temp))\n",
    "\n",
    "    return \"\\n\".join(contents[1:])\n",
    "\n",
    "if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':\n",
    "    response = urlopen(status['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
    "    data = json.loads(response.read())\n",
    "    text = data['results']['transcripts'][0]['transcript']\n",
    "    text = spk_sperator(data)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74faf4ac-d5ec-42b7-8f6b-b02988898b10",
   "metadata": {},
   "source": [
    "# 3. Post Call Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27963a63-4eb6-4b40-a88c-9396ad202c7e",
   "metadata": {},
   "source": [
    "## 3.1. Choice of models in Bedrock\n",
    "Choose FMs from Amazon, AI21 Labs and Anthropic to find the right FM for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14312ebe-27a4-4f83-9971-1f49637ad741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock, print_ww\n",
    "from utils.bedrock import bedrock_info\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "# os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b318133-e887-4e11-9ce4-2fd78a6ae6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - create the Anthropic Model\n",
    "llm = Bedrock(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V2\"),\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\":512,\n",
    "        \"stop_sequences\":[\"\\n\\nHuman:\", \"\\n\\n인간\", \"\\n\\n상담원\", \"\\n\\n\"],\n",
    "        \"temperature\":0,\n",
    "        \"top_p\":0.999,\n",
    "    },\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00ed0d-3400-403f-a40d-60bf22204ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_tokens = {\n",
    "    \"Claude\" : 100000,\n",
    "    \"TitanText\": 4096,\n",
    "    \"Claude-instant\": 9000,\n",
    "    \"Claude-V2\" : 100000,\n",
    "}\n",
    "\n",
    "max_tokens = {\"Claude\" : 120, \"TitanText\": 130, \"Claude-instant\": 120, \"Claude-V2\" : 120}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128dd79-f560-4278-82e7-19d1b2a16eac",
   "metadata": {},
   "source": [
    "## 3.2. Prompt Template\n",
    "이 노트북에서는 네 가지 분석(**요약, 감성, 의도, 해결**)을 수행하게 되며, 각 분석에 대한 템플릿이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceac2e-313a-4b3c-9cda-ca5359f68a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template_ko = \"\"\"\\n\\nHuman:\n",
    "아래의 리테일 지원 통화 기록을 분석하세요. 전체 문장으로 대화에 대한 자세한 요약을 제공하세요.\n",
    "\n",
    "통화: \"{transcript}\"\n",
    "\n",
    "요약:\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "question_template_ko = \"\"\"\\n\\nHuman:\n",
    "\n",
    "아래의 통화 기록을 바탕으로 질문에 답하세요.\n",
    "\"<시작시간>\" 이어지는 문장의 시작시간을 나타내고 \"<종료시간>\"은 앞 문장의 종료시간을 나타냅니다.\n",
    "\n",
    "통화: \"{transcript}\"\n",
    "\n",
    "질문: \"{question}\"\n",
    "\n",
    "응답:\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "question_time_template_ko = \"\"\"\\n\\nHuman:\n",
    "\n",
    "아래의 통화 기록을 바탕으로 질문에 답하세요.\n",
    "\"<시작시간>\" 이어지는 문장의 시작시간을 나타내고 \"<종료시간>\"은 앞 문장의 종료시간을 나타냅니다.\n",
    "\n",
    "통화: \"{transcript}\"\n",
    "\n",
    "질문: \"{question} 답변과 함께 답변을 위해 참조한 대화의 시작 및 종료시간을 아래 형태로 알려주세요. \\n답변:\\n시작시간:\\n종료시간:\"\n",
    "\n",
    "응답:\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54dae-6f83-4820-9764-d8e9b943f8cc",
   "metadata": {},
   "source": [
    "## 3.3. Generate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5fe77-773c-4b3c-b146-068be5ddade0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analysis(llm, transcript, params, question=None, template=\"\", max_tokens=50):\n",
    " \n",
    "    if question is None:\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "        analysis_prompt = prompt.format(transcript=transcript)\n",
    "    else:\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"transcript\", \"question\"])\n",
    "        analysis_prompt = prompt.format(\n",
    "            transcript=transcript,\n",
    "            question=question\n",
    "        )\n",
    "    llm.model_kwargs = params\n",
    "\n",
    "    print(colored(analysis_prompt, 'green'))\n",
    "    response = llm(analysis_prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_clues(res):\n",
    "    \n",
    "    def _extract_time(res):\n",
    "        for item in res.split(\"\\n\"):\n",
    "            if \"시작시간:\" in item:\n",
    "                start_time = item.replace(\"시작시간:\", \"\").strip()\n",
    "            elif \"종료시간:\" in item:\n",
    "                end_time = item.replace(\"종료시간:\", \"\").strip()\n",
    "\n",
    "        return start_time, end_time\n",
    "    \n",
    "    def _trim_audio_data(audio_file, save_file, start, end):\n",
    "        sr = 96000\n",
    "        y, sr = librosa.load(audio_file, sr=sr)\n",
    "        ny = y[sr*(int(float(start))-3):sr*(int(float(end))+3)]\n",
    "        sf.write(save_file, ny, 96000)\n",
    "    \n",
    "    start_time, end_time = _extract_time(res)\n",
    "    \n",
    "    print (start_time, end_time)\n",
    "    _trim_audio_data(\"./records/voice-examples.wav\", \"./records/clue/clue.wav\", start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6532f5-39c8-4e46-9752-fc76eeb37d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"max_tokens_to_sample\":512,\n",
    "    \"stop_sequences\":[\"\\n\\nHuman\", \"\\n\\n인간\", \"\\n\\n상담사\", \"\\n\\n\\n\", \"\\n\\n질문\"],\n",
    "    \"temperature\":0,\n",
    "    \"top_p\":0.999\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedaaac-c4ea-40b5-a4e8-02b6f913ebc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.1. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f47f6-953c-4c25-b928-60747e7b29ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    template=summary_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b317a2-f6ca-4c4d-981f-a8a29745e1ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.2. Question and Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe543092-4d06-44c1-991c-5d136d4b28af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "question = \"고객의 감정은 어떤가요?\"\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    question=question,\n",
    "    template=question_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ece112-fb70-48dd-982a-58bc74a72fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"문제에 대한 개선을 위해서 어떤 방법이 있을까요?\"\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    question=question,\n",
    "    template=question_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca68894-4069-41ac-a427-ab30190a5901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"학습지는 언제 종료되나요?\"\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    question=question,\n",
    "    template=question_time_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)\n",
    "get_clues(res)\n",
    "ipd.Audio(\"./records/clue/clue.wav\", autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e94f1-9b5b-41fb-8cb7-fa94428b5035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"환불은 언제 가능한가요?\"\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    question=question,\n",
    "    template=question_time_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)\n",
    "get_clues(res)\n",
    "ipd.Audio(\"./records/clue/clue.wav\", autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70aa93-b296-4c9d-bef9-1b33724ffe7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"결제된 금액은 얼마인가요?\"\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    question=question,\n",
    "    template=question_time_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)\n",
    "get_clues(res)\n",
    "ipd.Audio(\"./records/clue/clue.wav\", autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10078db3-be4c-4f4d-81a1-daaabee4001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"상담원의 이름은 무엇인가요?\"\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    question=question,\n",
    "    template=question_time_template_ko\n",
    ")\n",
    "\n",
    "if not llm.streaming: print (res)\n",
    "get_clues(res)\n",
    "ipd.Audio(\"./records/clue/clue.wav\", autoplay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988ceca-2edb-4bff-93fd-b6730c0d54e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.4. Handling long call transcripts\n",
    "LLM의 인풋 토큰 한도를 초과하는 긴 문서를 처리하는 방법을 다룹니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1845d4-a206-4716-a6f6-7d4853455553",
   "metadata": {},
   "source": [
    "* Map Reduce\n",
    "![nn](../../imgs/map_reduce.png)\n",
    "출처: https://brain.d.foundation/Engineering/AI/Workaround+with+OpenAI's+token+limit+with+Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28835009-a774-49ab-bef5-f6d778bc3504",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Refine\n",
    "![nn](../../imgs/refine.png)\n",
    "출처: https://brain.d.foundation/Engineering/AI/Workaround+with+OpenAI's+token+limit+with+Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86ca5e-971b-4034-a5ed-99f4292227bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52757f-c7db-4e7a-992a-a84588496e99",
   "metadata": {},
   "source": [
    "* prompting to divide and conquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29b44c-b97e-4c60-bc12-2cd63ac0bbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt_template = \"\"\"\\n\\nHuman:\n",
    "\n",
    "다음 통화를 간단하게 요약해 주세요.\n",
    "\n",
    "통화: {text}\n",
    "\n",
    "요약:\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "chuck_prompt_template = \"\"\"\\n\\nHuman:\n",
    "\n",
    "다음 통화를 간단하게 요약해 주세요.\n",
    "\n",
    "통화: {text}\n",
    "\n",
    "요약:\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chuck_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"\\n\\nHuman:\n",
    "\n",
    "다음 통화를 간단하게 요약해 주세요.\n",
    "\n",
    "통화: {text}\n",
    "\n",
    "요약:\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99cafd0-4b0b-489a-95fd-11ae1d3e9828",
   "metadata": {},
   "source": [
    "* summarize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d665871-0207-4ac1-823c-8d0a7bdf8e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_chain_init(chain_type, llm):\n",
    "\n",
    "\n",
    "    combine_prompt = PromptTemplate(\n",
    "        template=combine_prompt_template,\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "    if chain_type == \"STUFF\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    elif chain_type == \"MAP_REDUCE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=chunk_prompt,\n",
    "            combine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif chain_type == \"REFINE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=chunk_prompt,\n",
    "            refine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08500d1-2aed-4498-bb7e-66d936fc574f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_call_analysis(llm, transcript, params, chain_type=\"MAP_REDUCE\", max_tokens=50):\n",
    "\n",
    "    llm.model_kwargs = params\n",
    "    num_tokens = llm.get_num_tokens(transcript) #raise warnning\n",
    "\n",
    "    print (num_tokens, max_tokens)\n",
    "\n",
    "    if num_tokens > max_tokens:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            #separators=[\"\\n\\n\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n",
    "\n",
    "        summary_chain = summary_chain_init(\n",
    "            chain_type=chain_type, \n",
    "            llm=llm\n",
    "        )\n",
    "        response = summary_chain(\n",
    "            {\"input_documents\": docs}\n",
    "        )\n",
    "\n",
    "        print (\"Intermediate_steps: \\n\")\n",
    "        for idx, step in enumerate(response[\"intermediate_steps\"]):\n",
    "            print (colored(f'step {idx}: \\n', \"green\"))\n",
    "            print (colored(f'{step}\\n', \"green\"))\n",
    "\n",
    "        return response[\"output_text\"]\n",
    "\n",
    "    else:\n",
    "\n",
    "        prompt = PromptTemplate(template=stuff_prompt_template, input_variables=[\"text\"])\n",
    "        analysis_prompt = prompt.format(text=transcript)\n",
    "        print (colored(analysis_prompt, 'green'))\n",
    "\n",
    "        response = llm(analysis_prompt)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df0e49-a073-4028-81e5-552a4133118a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"max_tokens_to_sample\":512,\n",
    "    \"stop_sequences\":[\"\\n\\nhuman\", \"\\n\\n인간\", \"\\n\\n상담사\", \"\\n\\n\\n\", \"\\n\\n질문\", \"\\n\\nspk_0\", \"\\n\\n통화\"],\n",
    "    \"temperature\":0,\n",
    "    \"top_p\":0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ddcc7-16ee-4a2b-be63-6fbc14e0c2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = long_call_analysis(\n",
    "    llm=llm,\n",
    "    transcript=text,\n",
    "    params=PARAMS,\n",
    "    chain_type=\"REFINE\" # REFINE, MAP_REDUCE\n",
    ")\n",
    "\n",
    "\n",
    "if not llm.streaming:\n",
    "    print (\"Results: \\n\")\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a01b4d-86eb-4e75-b3ee-9ce523909c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
